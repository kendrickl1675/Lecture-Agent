import os
import glob
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
import time

# --- é…ç½® ---
SOURCE_DIR = "./test_notes"  # æŒ‡å‘ä½ çš„ç¬”è®°ç›®å½•
PERSIST_DIR = "./chroma_db"  # å‘é‡åº“å­˜å‚¨ä½ç½®
EMBEDDING_MODEL = "BAAI/bge-m3"  # ç›®å‰æœ€å¼ºçš„å¼€æºä¸­è‹±åŒè¯­æ¨¡å‹


def load_and_chunk_markdown(path):
    """
    ä¸¤é˜¶æ®µåˆ‡åˆ†ç­–ç•¥ï¼š
    1. è¯­ä¹‰å±‚çº§åˆ‡åˆ†ï¼šæŒ‰ H1/H2/H3 æ ‡é¢˜åˆ‡åˆ†ï¼Œä¿ç•™å±‚çº§å…ƒæ•°æ®ã€‚
    2. å­—ç¬¦é•¿åº¦åˆ‡åˆ†ï¼šå¦‚æœæŸä¸ªç« èŠ‚å†…å®¹è¿‡é•¿ï¼Œå†æŒ‰å­—ç¬¦å¼ºåˆ¶æˆªæ–­ã€‚
    """
    try:
        with open(path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥ {path}: {e}")
        return []

    # 1. ç»“æ„åŒ–åˆ‡åˆ† (ä¿ç•™ä¸Šä¸‹æ–‡)
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    md_header_splits = markdown_splitter.split_text(text)

    # 2. é•¿åº¦æ§åˆ¶ (é˜²æ­¢æŸä¸ªç« èŠ‚å†™äº†5000å­—ï¼Œè¶…è¿‡æ¨¡å‹çª—å£)
    # chunk_size=1000 tokens å¤§çº¦å¯¹åº” BGE-M3 çš„æœ€ä½³çª—å£
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    final_splits = text_splitter.split_documents(md_header_splits)

    # æ³¨å…¥æºæ–‡ä»¶è·¯å¾„å…ƒæ•°æ®
    for doc in final_splits:
        doc.metadata["source"] = path

    return final_splits


def main():
    # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (å…³é”®ï¼šä½¿ç”¨ CUDA)
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ {EMBEDDING_MODEL} åˆ° GPU (RTX 4070 Ti)...")
    start_time = time.time()

    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'},  # æ˜¾å¼æŒ‡å®š CUDA
        encode_kwargs={'normalize_embeddings': True}  # å½’ä¸€åŒ–æœ‰åŠ©äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
    )
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")

    # 2. è¯»å–å¹¶åˆ‡åˆ†æ–‡ä»¶
    md_files = glob.glob(os.path.join(SOURCE_DIR, "**/*.md"), recursive=True)
    all_splits = []

    print(f"ğŸ“‚ å‘ç° {len(md_files)} ä¸ª Markdown æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    for file_path in md_files:
        splits = load_and_chunk_markdown(file_path)
        all_splits.extend(splits)
        print(f"  - {os.path.basename(file_path)} -> åˆ‡åˆ†ä¸º {len(splits)} ä¸ªå—")

    if not all_splits:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¾›ç´¢å¼•ã€‚")
        return

    # 3. å­˜å…¥ ChromaDB
    print(f"ğŸ’¾ æ­£åœ¨å°† {len(all_splits)} ä¸ªå‘é‡å—å†™å…¥æ•°æ®åº“...")

    # å¦‚æœæ•°æ®åº“å·²å­˜åœ¨ï¼Œç›´æ¥è¿½åŠ ï¼›å¦åˆ™åˆ›å»º
    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=embeddings,
        persist_directory=PERSIST_DIR,
        collection_name="fintech_knowledge"
    )

    print(f"ğŸ‰ ç´¢å¼•æ„å»ºæˆåŠŸï¼æ•°æ®å·²æŒä¹…åŒ–è‡³ {PERSIST_DIR}")
    print(f"âœ… æ˜¾å­˜å ç”¨æç¤º: è¯·è§‚å¯Ÿä»»åŠ¡ç®¡ç†å™¨ï¼ŒBGE-M3 åº”è¯¥å ç”¨çº¦ 1-2GB VRAMã€‚")


if __name__ == "__main__":
    main()