import os
import logging
from typing import List, Optional
from tqdm import tqdm

# --- æ ¼å¼å¤„ç†åº“ ---
import pymupdf4llm  # PDF ç¥å™¨
from docx import Document
from pptx import Presentation
import pandas as pd

# --- LangChain ç»„ä»¶ ---
from langchain_core.documents import Document as LangchainDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

import torch

# --- é…ç½® ---
SOURCE_DIR = r"./attachments"  # ä½ çš„è¯¾ä»¶å­˜æ”¾ç›®å½•
DB_DIR = "./chroma_db"  # å‘é‡æ•°æ®åº“è·¯å¾„
CHUNK_SIZE = 800  # åˆ†å—å¤§å°
CHUNK_OVERLAP = 100  # é‡å éƒ¨åˆ†

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class DocumentConverter:
    """
    å¤šæ ¼å¼è½¬æ¢å™¨ï¼šå°†äºŒè¿›åˆ¶æ–‡ä»¶ç»Ÿä¸€è½¬æ¢ä¸ºæ¸…æ´—åçš„ Markdown æ–‡æœ¬
    """

    @staticmethod
    def convert_pdf(file_path: str) -> str:
        """ä½¿ç”¨ PyMuPDF4LLM å°† PDF è½¬æ¢ä¸º Markdown (ä¿ç•™è¡¨æ ¼ç»“æ„)"""
        try:
            # pymupdf4llm ç›´æ¥è¿”å› markdown å­—ç¬¦ä¸²
            md_text = pymupdf4llm.to_markdown(file_path)
            return md_text
        except Exception as e:
            logging.error(f"âŒ PDF Convert Error ({file_path}): {e}")
            return ""

    @staticmethod
    def convert_docx(file_path: str) -> str:
        """æå– Word æ–‡æ¡£å¹¶ä¿ç•™åŸºæœ¬ç»“æ„"""
        try:
            doc = Document(file_path)
            full_text = []
            for para in doc.paragraphs:
                if para.text.strip():
                    # ç®€å•çš„æ ‡é¢˜è¯†åˆ«é€»è¾‘
                    if para.style.name.startswith('Heading'):
                        full_text.append(f"## {para.text}")
                    else:
                        full_text.append(para.text)
            return "\n\n".join(full_text)
        except Exception as e:
            logging.error(f"âŒ DOCX Convert Error ({file_path}): {e}")
            return ""

    @staticmethod
    def convert_pptx(file_path: str) -> str:
        """æå– PPT å†…å®¹ï¼ŒæŒ‰å¹»ç¯ç‰‡åˆ†é¡µ"""
        try:
            prs = Presentation(file_path)
            full_text = []
            for i, slide in enumerate(prs.slides):
                slide_content = [f"## Slide {i + 1}"]

                # å°è¯•æå–æ ‡é¢˜
                if slide.shapes.title:
                    slide_content.append(f"### {slide.shapes.title.text}")

                # æå–æ­£æ–‡æ–‡æœ¬æ¡†
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        # é¿å…é‡å¤æ ‡é¢˜
                        if shape == slide.shapes.title:
                            continue
                        slide_content.append(shape.text)

                full_text.append("\n".join(slide_content))
            return "\n\n---\n\n".join(full_text)
        except Exception as e:
            logging.error(f"âŒ PPTX Convert Error ({file_path}): {e}")
            return ""

    @staticmethod
    def convert_excel(file_path: str) -> str:
        """
        é€šç”¨ Excel è½¬æ¢å™¨ï¼šæ”¯æŒ .xlsx (æ ‡å‡†), .xlsm (å¸¦å®), .xlsb (äºŒè¿›åˆ¶)
        """
        try:
            ext = os.path.splitext(file_path)[1].lower()

            # 1. æ™ºèƒ½é€‰æ‹©å¼•æ“
            engine = None
            if ext == '.xlsb':
                engine = 'pyxlsb'  # äºŒè¿›åˆ¶ä¸“ç”¨å¼•æ“
            else:
                engine = 'openpyxl'  # .xlsx å’Œ .xlsm ç”¨è¿™ä¸ª

            # 2. åŠ è½½æ–‡ä»¶
            xls = pd.ExcelFile(file_path, engine=engine)
            full_text = []

            for sheet_name in xls.sheet_names:
                # è¯»å–æ•°æ® (è‡ªåŠ¨å¿½ç•¥ .xlsm ä¸­çš„ VBA ä»£ç )
                df = pd.read_excel(xls, sheet_name=sheet_name)

                # 3. æ•°æ®æ¸…æ´— (è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰ä¼˜åŒ–çš„æ ¸å¿ƒ)
                df = df.fillna("")  # æ¸…æ´— NaN

                # æˆªæ–­è¿‡å¤§çš„è¡¨æ ¼ (é˜²æ­¢ Token çˆ†ç‚¸)
                if len(df) > 50:
                    df = df.head(50)
                    full_text.append(f"> [!WARNING] Table truncated (showing first 50 rows)")

                if not df.empty:
                    md_table = df.to_markdown(index=False)
                    full_text.append(f"## Sheet: {sheet_name}\n\n{md_table}")

            return "\n\n".join(full_text)

        except ImportError as e:
            if '.xlsb' in file_path:
                logging.error(f"âŒ Missing Library: Please run `uv pip install pyxlsb` to read .xlsb files.")
            return ""
        except Exception as e:
            logging.error(f"âŒ Excel Convert Error ({file_path}): {e}")
            return ""


class KnowledgeIndexer:
    def __init__(self):
        if torch.cuda.is_available():
            device_type = 'cuda'
        elif torch.backends.mps.is_available():
            device_type = 'mps'  # Apple Silicon çš„åŠ é€Ÿå™¨
        else:
            device_type = 'cpu'

        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': device_type}
        )

        self.converter = DocumentConverter()

        # æ–‡æœ¬åˆ†å—å™¨ (é’ˆå¯¹ Markdown ä¼˜åŒ–)
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n## ", "\n### ", "\n", " ", ""]  # ä¼˜å…ˆæŒ‰æ ‡é¢˜åˆ‡åˆ†
        )

    def process_directory(self, source_dir: str):
        """æ‰«æå¹¶å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶"""
        if not os.path.exists(source_dir):
            os.makedirs(source_dir)
            print(f"ğŸ“‚ Created directory: {source_dir}. Put your files here!")
            return

        all_docs = []
        files = [f for f in os.listdir(source_dir) if not f.startswith("~")]  # å¿½ç•¥ä¸´æ—¶æ–‡ä»¶

        print(f"ğŸ” Found {len(files)} files. Starting conversion...")

        for filename in tqdm(files, desc="Converting"):
            file_path = os.path.join(source_dir, filename)
            ext = os.path.splitext(filename)[1].lower()

            content = ""
            if ext == ".pdf":
                content = self.converter.convert_pdf(file_path)
            elif ext == ".docx":
                content = self.converter.convert_docx(file_path)
            elif ext == ".pptx":
                content = self.converter.convert_pptx(file_path)
            elif ext in [".xlsx", ".xlsm", ".xlsb", ".xls"]:
                content = self.converter.convert_excel(file_path)
            elif ext == ".md":
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            else:
                logging.warning(f"âš ï¸ Skipped unsupported format: {filename}")
                continue

            if content:
                # å°è£…ä¸º LangChain Documentï¼Œå¸¦ä¸Šå…ƒæ•°æ®
                doc = LangchainDocument(
                    page_content=content,
                    metadata={"source": filename, "type": ext}
                )
                all_docs.append(doc)

        if not all_docs:
            print("âš ï¸ No valid content extracted.")
            return

        # åˆ†å—
        print(f"âœ‚ï¸ Splitting {len(all_docs)} documents...")
        chunks = self.splitter.split_documents(all_docs)
        print(f"ğŸ§© Generated {len(chunks)} chunks.")

        # å­˜å…¥æ•°æ®åº“
        print("ğŸ’¾ Persisting to ChromaDB (this may take a while)...")
        vectordb = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=DB_DIR,
            collection_name="fintech_knowledge"
        )
        print("âœ… Indexing Complete! Your agent can now read your course materials.")


if __name__ == "__main__":
    indexer = KnowledgeIndexer()
    indexer.process_directory(SOURCE_DIR)