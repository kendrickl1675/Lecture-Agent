import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

load_dotenv()


class LectureAgentCore:
    def __init__(self):
        # æ‰“å°å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œæ–¹ä¾¿è°ƒè¯•ç¡®è®¤
        print(f"ğŸ§  åˆå§‹åŒ– Agent (Engine: {os.getenv('MODEL_NAME')})...")

        # 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ (RAG è®°å¿†æ¨¡å—)
        # ä½¿ç”¨ BAAI/bge-m3 æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆ
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': 'cuda'}  # å¼ºåˆ¶ä½¿ç”¨ GPU åŠ é€Ÿ
        )
        # åŠ è½½æœ¬åœ°æŒä¹…åŒ–çš„æ•°æ®åº“
        self.vector_db = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings,
            collection_name="fintech_knowledge"
        )

        # 2. LLM åˆå§‹åŒ– (å¤§è„‘)
        # æ¸©åº¦è®¾ä¸º 0.1 ä»¥ä¿è¯å­¦æœ¯è¾“å‡ºçš„ä¸¥è°¨æ€§å’Œä¸€è‡´æ€§
        self.llm = ChatGoogleGenerativeAI(
            model=os.getenv("MODEL_NAME", "gemini-3-flash-preview"),  # å»ºè®®åœ¨ .env ä¸­ç®¡ç†ç‰ˆæœ¬
            google_api_key=os.getenv("GOOGLE_API_KEY"),
            temperature=0.1,
        )

        # 3. System Prompt (æ ¸å¿ƒæŒ‡ä»¤é›†)
        # åŒ…å«ï¼šè§’è‰²å®šä¹‰ã€RAGä¸Šä¸‹æ–‡æ³¨å…¥ã€ä»»åŠ¡æŒ‡ä»¤ã€é˜²å¾¡æœºåˆ¶ã€æ ¼å¼çº¦æŸ
        self.system_prompt = """
        You are a strict academic research assistant in Quantitative Finance.

        [CONTEXT FROM LOCAL DATABASE]
        (This provides specific lecture details. It might be empty if no prior notes exist.)
        {context}

        [TASK]
        Refine the [USER INPUT] into rigorous **Academic English** Markdown based on TWO sources: 
        1. The [CONTEXT] provided above.
        2. Your own **Internal Expert Knowledge** of Finance/Math/Coding.

        [GATEKEEPING RULES]
        - **CASE A (Chat/Nonsense):** If the input is purely conversational or lacks technical substance, output EXACTLY: "SKIP_PROCESSING".
        - **CASE B (Valid Content):** If input contains recognizable concepts, process it even if context is empty.

        [CRITICAL RULES]
        1. **PROTECTED TOKENS**: You will see placeholders like `__IMG_0__` or `__LINK_1__`. **DO NOT CHANGE, DELETE, OR MOVE THEM.**
        2. **TERM ANALYSIS (CONSTRAINTS)**:
                   - **Quantity Limit**: TOP 3-5 critical terms only.
                   - **Expansion Logic**: Strictly relevant to current context.
                   - **Location**: Place "Key Term Analysis" at the very END.

                   **Format:**
                   ### ğŸ†Key Term Analysis
                   * **[Term Name]**
                       * **Origin**: ...
                       * **Application**: ...
                       * **Expansion**: ...
        3. **OBSIDIAN CALLOUT PRESERVATION**: 
           - Reconstruct Callouts (`> [!NOTE]`) exactly.
           - Academic text must be inside blockquote (`> ` prefix).
        4. **Math**: Use LaTeX ($...$).
        5. **Regulations**: Prioritize AMCM/PBOC/HKMA.

        [USER INPUT]
        {input_text}
        """

        self.prompt = ChatPromptTemplate.from_template(self.system_prompt)
        self.chain = self.prompt | self.llm | StrOutputParser()

    def generate_note(self, raw_text):
        # 1. è¾“å…¥é¢„æ£€æŸ¥ï¼šå¤ªçŸ­çš„æ–‡æœ¬ç›´æ¥å¿½ç•¥ï¼ŒèŠ‚çœ API è°ƒç”¨
        if not raw_text or len(raw_text.strip()) < 3:
            return raw_text

        # 2. RAG æ£€ç´¢æµç¨‹
        try:
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©ºï¼Œé˜²æ­¢å†·å¯åŠ¨æŠ¥é”™
            if self.vector_db._collection.count() == 0:
                context_str = "No local context available (Database is empty)."
            else:
                # ä½¿ç”¨å¸¦é˜ˆå€¼çš„æ£€ç´¢ï¼Œè¿‡æ»¤æ‰ç›¸å…³æ€§ä½çš„å†…å®¹
                retriever = self.vector_db.as_retriever(
                    search_type="similarity_score_threshold",
                    search_kwargs={"score_threshold": 0.6, "k": 2}
                )
                docs = retriever.invoke(raw_text)
                if docs:
                    context_str = "\n".join([f"- {d.page_content}" for d in docs])
                else:
                    context_str = "No relevant context found in local database."

        except Exception as e:
            # æ£€ç´¢å¤±è´¥ä¸åº”é˜»æ–­ä¸»æµç¨‹ï¼Œé™çº§ä¸ºæ—  RAG æ¨¡å¼
            context_str = f"Context retrieval skipped: {str(e)}"

        # 3. LLM ç”Ÿæˆæµç¨‹
        try:
            response = self.chain.invoke({
                "context": context_str,
                "input_text": raw_text
            })

            # 4. é²æ£’æ€§æ£€æŸ¥ï¼šå¦‚æœæ¨¡å‹åˆ¤æ–­ä¸ºé—²èŠï¼Œåˆ™åŸæ ·è¿”å›
            if "SKIP_PROCESSING" in response:
                print(f"â­ï¸  Skipped: {raw_text[:30]}... (Chat/Nonsense)")
                return raw_text

            return response

        except Exception as e:
            print(f"âŒ Error: {e}")
            return raw_text